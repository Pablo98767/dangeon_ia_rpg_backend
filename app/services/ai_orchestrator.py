import os
import json
from typing import List, Dict, Optional
import httpx
from fastapi import HTTPException

# ==========================================
# CONFIGURA√á√ïES - GPT-4o-mini via OpenRouter
# ==========================================
SITE_URL = os.getenv("OPENROUTER_SITE_URL", "http://localhost:8000")
SITE_NAME = os.getenv("OPENROUTER_SITE_NAME", "RPG-IA-Backend")
API_KEY = os.getenv("OPENROUTER_API_KEY")

# üöÄ MODELO ATUALIZADO: GPT-4o-mini (muito mais poderoso!)
MODEL = "openai/gpt-4o-mini"

# ==========================================
# SYSTEM PROMPT OTIMIZADO
# ==========================================
SYSTEM_PROMPT = """
Voc√™ √© um narrador de hist√≥rias interativas de RPG extremamente criativo e envolvente.
Responda SEMPRE em JSON estrito, no formato:
{
  "text": "trecho da hist√≥ria em portugu√™s",
  "choices": ["op√ß√£o 1", "op√ß√£o 2", "op√ß√£o 3", "op√ß√£o 4"]
}

REGRAS IMPORTANTES:
- "text": Deve conter uma narrativa envolvente, descritiva e imersiva (200-400 palavras)
- "choices": Deve ter entre 2 e 4 op√ß√µes curtas, claras e mutuamente exclusivas
- Nunca inclua coment√°rios fora do JSON. Nunca use markdown. Apenas JSON puro.
- Mantenha a coer√™ncia da hist√≥ria, personagens consistentes e l√≥gica interna impec√°vel
- Crie hist√≥rias com in√≠cio, desenvolvimento e conclus√µes √©picas
- Use elementos dram√°ticos, reviravoltas e momentos emocionantes
- Adapte-se ao tom e tema escolhido pelo jogador

IMPORTANTE: Responda APENAS com o objeto JSON, sem nenhum texto adicional antes ou depois.
"""

# ==========================================
# FUN√á√ïES AUXILIARES DE PARSING JSON
# ==========================================

def _extract_last_json_object(text: str) -> Optional[str]:
    """
    Extrai o √∫ltimo objeto JSON v√°lido de uma string,
    mesmo que contenha texto antes ou depois.
    """
    brace_stack = 0
    in_string = False
    escape = False
    start_idx = None
    last_obj = None

    for i, ch in enumerate(text):
        if in_string:
            if escape:
                escape = False
            elif ch == "\\":
                escape = True
            elif ch == '"':
                in_string = False
            continue
        else:
            if ch == '"':
                in_string = True
                continue
            if ch == '{':
                if brace_stack == 0:
                    start_idx = i
                brace_stack += 1
            elif ch == '}':
                if brace_stack > 0:
                    brace_stack -= 1
                    if brace_stack == 0 and start_idx is not None:
                        last_obj = text[start_idx:i+1]
                        start_idx = None
    return last_obj


def _parse_json_strict(content: str) -> Dict:
    """
    Tenta fazer o parse do JSON de forma inteligente.
    Primeiro tenta parse direto, depois procura por objeto JSON,
    e por √∫ltimo retorna fallback com escolhas padr√£o.
    """
    # Tentativa 1: Parse direto
    try:
        data = json.loads(content)
        if isinstance(data, dict) and "text" in data and isinstance(data.get("choices"), list):
            return data
    except Exception:
        pass

    # Tentativa 2: Extrair √∫ltimo objeto JSON v√°lido
    blob = _extract_last_json_object(content)
    if blob:
        try:
            data = json.loads(blob)
            if isinstance(data, dict) and "text" in data and isinstance(data.get("choices"), list):
                return data
        except Exception:
            pass

    # Fallback: Retorna o texto completo com escolhas padr√£o
    return {
        "text": content.strip(),
        "choices": ["Seguir adiante", "Recuar com cautela", "Investigar ao redor"]
    }


# ==========================================
# FUN√á√ÉO DE CHAMADA √Ä IA (GPT-4o-mini)
# ==========================================

async def _chat_once(user_prompt: str) -> Dict:
    """
    Faz uma chamada √∫nica √† API da OpenRouter usando GPT-4o-mini.
    Retorna um dicion√°rio com 'text' e 'choices'.
    """
    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {API_KEY}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": SITE_URL,
                    "X-Title": SITE_NAME,
                },
                json={
                    "model": MODEL,
                    "messages": [
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.8,  # Criatividade balanceada
                    "max_tokens": 800,   # Permite respostas mais elaboradas
                }
            )
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="Timeout ao chamar a IA")
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"Erro de conex√£o com a IA: {str(e)}")

    if response.status_code != 200:
        raise HTTPException(
            status_code=503, 
            detail=f"Erro ao chamar IA: {response.status_code} - {response.text}"
        )

    # Parse da resposta
    response_data = response.json()
    content = response_data["choices"][0]["message"]["content"]
    
    # Parse inteligente do JSON
    data = _parse_json_strict(content)
    
    # Valida√ß√£o e normaliza√ß√£o das escolhas
    choices = data.get("choices") or []
    if len(choices) < 2:
        choices = ["Seguir adiante", "Recuar com cautela", "Investigar ao redor"]
    elif len(choices) > 4:
        choices = choices[:4]
    
    return {
        "text": (data.get("text") or "").strip(),
        "choices": choices
    }


# ==========================================
# FUN√á√ÉO PRINCIPAL DE GERA√á√ÉO DE HIST√ìRIA
# ==========================================

async def generate_next_step(
    theme: str, 
    character: str, 
    history: List[dict], 
    max_choices: int = 4
) -> Dict:
    """
    Gera o pr√≥ximo passo da hist√≥ria baseado no tema, personagem e hist√≥rico.
    
    Args:
        theme: Tema/g√™nero da hist√≥ria (fantasia, sci-fi, terror, etc)
        character: Descri√ß√£o do personagem principal
        history: Lista de eventos anteriores da hist√≥ria
        max_choices: N√∫mero m√°ximo de escolhas (2-4)
    
    Returns:
        Dict com: index, text, choices, model
    """
    
    # Construir resumo do hist√≥rico (√∫ltimas 3 intera√ß√µes)
    recap = ""
    if history:
        recap = "üìú RESUMO DOS EVENTOS RECENTES:\n"
        for h in history[-3:]:
            chosen_idx = h.get("chosen_index")
            tag = "" if chosen_idx is None else f" [Escolha: {chosen_idx + 1}]"
            recap += f"‚Ä¢ {h['text'][:250]}{tag}\n"
    
    # Construir prompt para a IA
    user_prompt = f"""
üéÆ GERA√á√ÉO DE HIST√ìRIA INTERATIVA

üìñ TEMA: {theme}
üë§ PERSONAGEM: {character}

{recap}

üéØ TAREFA:
Gere o pr√≥ximo trecho emocionante da hist√≥ria com no m√°ximo {max_choices} escolhas de a√ß√£o.

LEMBRE-SE:
- Crie uma narrativa envolvente e imersiva
- As escolhas devem ser interessantes e impactantes
- Mantenha a coer√™ncia com os eventos anteriores
- Responda APENAS com JSON no formato especificado
"""

    # Calcular o √≠ndice do pr√≥ximo passo
    last_index = history[-1]["index"] + 1 if history else 0

    try:
        # Chamada √† IA
        result = await _chat_once(user_prompt)
        
        # Normalizar n√∫mero de escolhas
        result["choices"] = (result["choices"] or [])[:max(2, min(4, max_choices))]
        
        # Retornar resposta estruturada
        return {
            "index": last_index,
            "text": result["text"],
            "choices": result["choices"],
            "model": MODEL,
        }
        
    except HTTPException:
        raise  # Re-lan√ßa HTTPExceptions
    except Exception as e:
        raise HTTPException(
            status_code=503, 
            detail=f"Servi√ßo de IA indispon√≠vel: {type(e).__name__} - {str(e)}"
        )


# ==========================================
# INFORMA√á√ïES DO MODELO (para logging/debug)
# ==========================================

def get_model_info() -> Dict:
    """
    Retorna informa√ß√µes sobre o modelo atual.
    """
    return {
        "model": MODEL,
        "provider": "OpenRouter",
        "base_model": "OpenAI GPT-4o-mini",
        "context_window": "128k tokens",
        "pricing": {
            "input": "$0.15 per 1M tokens",
            "output": "$0.60 per 1M tokens"
        }
    }